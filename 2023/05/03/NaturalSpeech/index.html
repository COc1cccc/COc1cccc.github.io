

<!DOCTYPE html>
<html lang="en" color-mode=light>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>NaturalSpeech 2 Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers - Avidya</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="google" content="notranslate" />
  
  <meta name="description" content="NaturalSpeech 2 Latent Diff...">
  <meta name="author" content="Kou">
  <link rel="icon" href="/images/icons/favicon-32x32.png" type="image/png" sizes="16x16">
  <link rel="icon" href="/images/icons/favicon-64x64.png" type="image/png" sizes="32x32">
  <link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180">
  <meta rel="mask-icon" href="/images/icons/stun-logo.svg" color="#333333">
  
    <meta rel="msapplication-TileImage" content="/images/icons/favicon-144x144.png">
    <meta rel="msapplication-TileColor" content="#000000">
  

  
<link rel="stylesheet" href="/css/style.css">


  
    
<link rel="stylesheet" href="https://at.alicdn.com/t/font_1445822_p6ry5n7lrr.css">

  

  
    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css">

  

  
    
      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/foundation.min.css" name="highlight-style" mode="light">

      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/solarized-dark.min.css" name="highlight-style" mode="dark">

      
  

  <script>
    var CONFIG = window.CONFIG || {};
    var ZHAOO = window.ZHAOO || {};
    CONFIG = {
      isHome: false,
      fancybox: true,
      pjax: false,
      loading: {
        gif: '/images/theme/loading.gif',
        lottie: ''
      },
      lazyload: {
        enable: true,
        only_post: 'false',
        loading: {
          gif: '/images/theme/loading.gif',
          lottie: ''
        }
      },
      donate: {
        enable: false,
        alipay: '',
        wechat: ''
      },
      galleries: {
        enable: false
      },
      fab: {
        enable: true,
        always_show: false
      },
      carrier: {
        enable: true
      },
      daovoice: {
        enable: false
      },
      preview: {
        background: {
          default: '',
          api: ''
        },
        motto: {
          default: 'もしも僕が主人公なら、僕は人の心が見えて',
          typing: true,
          api: '',
          data_contents: '["data","content"]'
        },
      },
      qrcode: {
        enable: false,
        type: 'url',
        image: '/images/theme/qr-code.png',
      },
      toc: {
        enable: true
      },
      scrollbar: {
        type: 'default'
      },
      notification: {
        enable: false,
        delay: 4500,
        list: '',
        page_white_list: '',
        page_black_list: ''
      },
      search: {
        enable: false,
        path: ''
      }
    }
  </script>

  

  

<meta name="generator" content="Hexo 6.3.0"></head>

<body class="lock-screen">
  <div class="loading" id="loading"></div>
  
    


  <nav class="navbar">
    <div class="left">
      
        <i class="iconfont iconhome j-navbar-back-home"></i>
      
      
      
        <i class="iconfont iconmoono" id="color-toggle" color-toggle="light"></i>
      
      
    </div>
    <div class="center">NaturalSpeech 2 Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers</div>
    <div class="right">
      <i class="iconfont iconmenu j-navbar-menu"></i>
    </div>
    
  </nav>

  
  

<nav class="menu">
  <div class="menu-container">
    <div class="menu-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <ul class="menu-content"><li class="menu-item">
        <a href="/ " class="underline "> home</a>
      </li><li class="menu-item">
        <a href="/archives/ " class="underline "> archives</a>
      </li><li class="menu-item">
        <a href="/about/ " class="underline "> about</a>
      </li></ul>
    
  </div>
</nav>
  <main id="main">
  <div class="article-wrap">
    <div class="row container">
      <div class="col-xl-3"></div>
      <div class="col-xl-6"><article class="article">
  <div class="wrap">
    <section class="head">
  <img   class="lazyload" data-original="/images/theme/post-image.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="  draggable="false">
  <div class="head-mask">
    <h1 class="head-title">NaturalSpeech 2 Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers</h1>
    <div class="head-info">
      <span class="post-info-item"><i class="iconfont iconcalendar"></i>May 03, 2023</span>
      
      <span class="post-info-item"><i class="iconfont iconfont-size"></i>11856</span>
    </div>
  </div>
</section>
    <section class="main">
      <section class="content">
        
        <p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.09116">NaturalSpeech 2 Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers</a><br><a target="_blank" rel="noopener" href="https://github.com/lucidrains/naturalspeech2-pytorch">https://github.com/lucidrains/naturalspeech2-pytorch</a><br><a target="_blank" rel="noopener" href="https://speechresearch.github.io/naturalspeech2/">https://speechresearch.github.io/naturalspeech2/</a></p>
<p><img    class="lazyload" data-original="/images%5Ctheme%5Cns-1.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">Structure</span></p>
<p>A neural audio codec with a Residual Vector Quantizer (RVQ) is utilized to obtain quantized latent vectors, which are then used in a diffusion model to generate these text-based latent vectors.</p>
<p>To enhance zero-shot capabilities for diversified speech synthesis, a speech prompt mechanism (referencing audio) is designed to facilitate contextual learning in the diffusion model and duration&#x2F;pitch prediction.</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Current large-scale TTS systems like “Neural codec language models are zero-shot text to speech synthesizers,” “Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision,” and “FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model” typically convert continuous speech waveforms into discrete tokens and model these tokens using autoregressive language models.</p>
<p>The speech (discrete tokens) sequences are often lengthy (a 10-second speech usually has thousands of discrete tokens), and autoregressive models suffer from error propagation, resulting in unstable speech output. There exists a dilemma between the encoder-decoder and language model: on one hand, encoder-decoders with token quantization (e.g., VQ-VAE or VQ-GAN) typically yield low bit-rate token sequences, simplifying language model generation but leading to loss of high-frequency acoustic details; on the other hand, some improved approaches like “SoundStream: An End-to-End Neural Audio Codec” and “High Fidelity Neural Audio Compression” use multiple residual discrete tokens to represent speech frames (RVQ), which, when flattened, increase the length of token sequences several-fold and pose difficulties in language modeling.</p>
<p>This paper proposes NaturalSpeech 2, a TTS system utilizing a latent diffusion model. Firstly, a neural audio codec is trained, which converts speech waveforms into a series of latent vectors using an encoder-decoder and reconstructs speech waveforms from these latent vectors using a decoder.<br><img    class="lazyload" data-original="/images%5Ctheme%5Cns-2.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">First training stage</span><br>After training the audio encoder and decoder, we use the encoder-decoder to extract latent vectors from the speech in the training set. These latent vectors are then utilized as targets for the latent diffusion model, which conditions on previous vectors obtained from the phoneme encoder, duration predictor, and pitch predictor. During inference, we first generate latent vectors from the text&#x2F;phoneme sequence using the latent diffusion model and then generate speech waveforms from these latent vectors using the encoder-decoder.<br><img    class="lazyload" data-original="/images%5Ctheme%5Cns-3.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">Second training stage</span></p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><h2 id="TTS-for-Natural-Voice-Quality-and-Diversity"><a href="#TTS-for-Natural-Voice-Quality-and-Diversity" class="headerlink" title="TTS for Natural Voice: Quality and Diversity"></a>TTS for Natural Voice: Quality and Diversity</h2><p>Speech features can be represented in both continuous and discrete forms. In current TTS systems, neural encoders and decoders are typically employed to convert speech waveforms into discrete token sequences, which are then used by autoregressive language models to generate speech from text. However, this approach faces the following dilemmas:</p>
<ul>
<li><p>If the audio encoder-decoder utilizes a vector quantizer (VQ) to quantize each speech frame into a single token, the sequence length is short, simplifying token generation in the language model. However, this can lead to compromised waveform reconstruction quality due to high compression rates or low bit rates.</p>
</li>
<li><p>If the audio encoder-decoder uses a residual vector quantizer (RVQ) to quantize each speech frame into multiple tokens. However, the increased length of token sequences can make autoregressive model generation challenging, leading to issues with error propagation and robustness.</p>
</li>
</ul>
<p>These challenges highlight the need for innovative approaches in TTS to balance between waveform reconstruction quality and the stability of autoregressive model generation.</p>
<p>When employing autoregressive generation, it involves discretizing speech, which, when using a transformer architecture, requires transforming continuous speech data frame by frame into discrete tokens via VQ. However, post-VQ data tends to have low information entropy, leading to a decrease in generation quality. Alternatively, using MVQ can result in a complex and unstable generation process.</p>
<p>Adopting a two-stage coarse-to-fine generation approach may lead to error propagation and suboptimal results.</p>
<p>While residual vector quantizers can achieve good reconstruction quality and low bit rates, they are primarily designed for compression and transmission purposes and may not be suitable as intermediate representations for speech&#x2F;audio generation. The discrete symbol sequences generated by RVQ are often long (by a factor of R if using R VQs), making them challenging for language models to predict. When reconstructing speech waveforms from these tokens, inaccurately predicted discrete tokens may lead to issues such as word skipping, word repetition, or speech breakdown.</p>
<h2 id="Neural-Audio-Codec"><a href="#Neural-Audio-Codec" class="headerlink" title="Neural Audio Codec"></a>Neural Audio Codec</h2><p>A neural audio codec refers to a neural network model that utilizes an encoder-decoder to convert audio waveforms into a compact representation (which can be either continuous or discrete) and reconstructs audio waveforms from these representations using a decoder.<br>Due to the issues associated with discrete representations, this paper proposes a neural audio codec that converts speech waveforms into continuous vectors instead of discrete tokens. This approach aims to maintain sufficient granularity and detail for accurate waveform reconstruction without increasing the length of the sequence. Specifically, it utilizes a large number of quantizers (the middle part between the encoder and decoder) and residual vector quantizers (RVQ) to approximate continuous vectors.</p>
<h1 id="NaturalSpeech-2"><a href="#NaturalSpeech-2" class="headerlink" title="NaturalSpeech 2"></a>NaturalSpeech 2</h1><h2 id="Neural-Audio-Codec-with-Continuous-Vectors"><a href="#Neural-Audio-Codec-with-Continuous-Vectors" class="headerlink" title="Neural Audio Codec with Continuous Vectors"></a>Neural Audio Codec with Continuous Vectors</h2><p>Using a neural audio codec to convert speech waveforms into continuous vectors instead of discrete tokens offers several advantages:</p>
<ul>
<li><p>Continuous vectors have lower compression rates and higher bit rates compared to discrete tokens, ensuring high-quality audio reconstruction.</p>
</li>
<li><p>Each audio frame is represented by a single vector, as opposed to multiple tokens with discrete quantization, which does not increase the length of the hidden sequence.<br><img    class="lazyload" data-original="/images%5Ctheme%5Cns-4.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">Second training stage</span><br>The neural audio codec comprises an audio encoder, a residual vector quantizer (RVQ), and an audio decoder:</p>
</li>
<li><p>The audio encoder consists of several convolutional blocks, resulting in a total downsampling rate of 200 for 16 kHz audio, where each frame corresponds to a 12.5 ms segment of speech.</p>
</li>
<li><p>The residual vector quantizer converts the output of the audio encoder into multiple residual vectors. The sum of these residual vectors serves as the quantized vector, which is used as the training target for the diffusion model.</p>
</li>
<li><p>The audio decoder mirrors the structure of the audio encoder, generating audio waveforms from the quantized vector. The workflow of the neural audio codec is as follows.<br>MATHJAX-SSR-27<br>Where <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.836ex" height="2.509ex" style="vertical-align: -0.671ex;" viewBox="0 -791.3 1651.5 1080.4" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">f_{enc}</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMATHI-66" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path>
<path stroke-width="1" id="E1-MJMATHI-65" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path>
<path stroke-width="1" id="E1-MJMATHI-6E" d="M21 287Q22 293 24 303T36 341T56 388T89 425T135 442Q171 442 195 424T225 390T231 369Q231 367 232 367L243 378Q304 442 382 442Q436 442 469 415T503 336T465 179T427 52Q427 26 444 26Q450 26 453 27Q482 32 505 65T540 145Q542 153 560 153Q580 153 580 145Q580 144 576 130Q568 101 554 73T508 17T439 -10Q392 -10 371 17T350 73Q350 92 386 193T423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 180T152 343Q153 348 153 366Q153 405 129 405Q91 405 66 305Q60 285 60 284Q58 278 41 278H27Q21 284 21 287Z"></path>
<path stroke-width="1" id="E1-MJMATHI-63" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMATHI-66" x="0" y="0"></use>
<g transform="translate(490,-150)">
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-65" x="0" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-6E" x="466" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-63" x="1067" y="0"></use>
</g>
</g>
</svg>, <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.667ex" height="2.843ex" style="vertical-align: -1.005ex;" viewBox="0 -791.3 1578.7 1223.9" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">f_{rvq}</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMATHI-66" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path>
<path stroke-width="1" id="E1-MJMATHI-72" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path>
<path stroke-width="1" id="E1-MJMATHI-76" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path>
<path stroke-width="1" id="E1-MJMATHI-71" d="M33 157Q33 258 109 349T280 441Q340 441 372 389Q373 390 377 395T388 406T404 418Q438 442 450 442Q454 442 457 439T460 434Q460 425 391 149Q320 -135 320 -139Q320 -147 365 -148H390Q396 -156 396 -157T393 -175Q389 -188 383 -194H370Q339 -192 262 -192Q234 -192 211 -192T174 -192T157 -193Q143 -193 143 -185Q143 -182 145 -170Q149 -154 152 -151T172 -148Q220 -148 230 -141Q238 -136 258 -53T279 32Q279 33 272 29Q224 -10 172 -10Q117 -10 75 30T33 157ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMATHI-66" x="0" y="0"></use>
<g transform="translate(490,-150)">
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-72" x="0" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-76" x="451" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-71" x="937" y="0"></use>
</g>
</g>
</svg>, and <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.709ex" height="2.509ex" style="vertical-align: -0.671ex;" viewBox="0 -791.3 1597.1 1080.4" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">f_{dec}</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMATHI-66" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path>
<path stroke-width="1" id="E1-MJMATHI-64" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path>
<path stroke-width="1" id="E1-MJMATHI-65" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path>
<path stroke-width="1" id="E1-MJMATHI-63" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMATHI-66" x="0" y="0"></use>
<g transform="translate(490,-150)">
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-64" x="0" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-65" x="523" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-63" x="990" y="0"></use>
</g>
</g>
</svg> represent the audio encoder, residual vector quantizer, and audio decoder respectively. x is the speech waveform, h is the hidden sequence obtained by the audio encoder with frame length n, and z is the quantized vector sequence of the same length as h. i denotes the index of the speech frame, j is the index of RVQ, R is the total number of RVQs, and <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.883ex" height="2.676ex" style="vertical-align: -0.338ex;" viewBox="0 -1006.6 810.8 1152.1" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">e^i</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMATHI-65" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path>
<path stroke-width="1" id="E1-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMATHI-65" x="0" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-69" x="659" y="583"></use>
</g>
</svg>, j is the embedding vector of the codebook ID obtained by the j-th RVQ on the i-th hidden frame (i.e., <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.139ex" height="2.676ex" style="vertical-align: -0.338ex;" viewBox="0 -1006.6 920.8 1152.1" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">h^i</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMATHI-68" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path>
<path stroke-width="1" id="E1-MJMATHI-69" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMATHI-68" x="0" y="0"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJMATHI-69" x="815" y="583"></use>
</g>
</svg>).</p>
</li>
</ul>
<h2 id="Latent-Diffusion-Model-with-Non-Autoregressive-Generation"><a href="#Latent-Diffusion-Model-with-Non-Autoregressive-Generation" class="headerlink" title="Latent Diffusion Model with Non-Autoregressive Generation"></a>Latent Diffusion Model with Non-Autoregressive Generation</h2><p>Using a diffusion model to predict quantized feature vectors z conditioned on a text sequence y. We utilize a prior model composed of a phoneme encoder, duration predictor, and pitch predictor to process the text input and provide additional information in the form of a hidden vector c to serve as the condition for the diffusion model.<br><img    class="lazyload" data-original="/images%5Ctheme%5Cns-5.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">Second training stage</span></p>
<h2 id="Speech-Prompting-for-In-Context-Learning"><a href="#Speech-Prompting-for-In-Context-Learning" class="headerlink" title="Speech Prompting for In-Context Learning"></a>Speech Prompting for In-Context Learning</h2><p>To facilitate context learning for improved zero-shot generation, the authors devised a speech prompt mechanism to encourage the duration&#x2F;pitch predictor and diffusion model to follow different cues (e.g. speaker identity).</p>
<p>For a speech latent sequence z, we randomly extract a segment z[u:v] by frame indices from u to v as the speech prompt, and concatenate the remaining speech segments z[1:u] and z[v:n] to form a new sequence z\u:v as the learning target for the diffusion model.<br><img    class="lazyload" data-original="/images%5Ctheme%5Cns-6.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="   ><span class="image-caption">Second training stage</span></p>
<p>As shown in Figure, the authors employed a transformer-based prompt encoder to handle the speech prompt z[u:v] (referred to as zp in the figure) to obtain a hidden sequence. To utilize this hidden sequence as a prompt, two different strategies were employed for the duration&#x2F;pitch predictor and diffusion model:</p>
<ul>
<li>For the duration and pitch predictor, a Q-K-V attention layer was inserted into the convolutional layer, where the query was the hidden sequence of the convolutional layer, and the keys and values were from the hidden sequence of the prompt encoder.</li>
<li>For the diffusion model, two attention blocks were designed instead of directly focusing on the hidden sequence from the prompt encoder, as it may expose too many details to the diffusion model, potentially compromising generation: In the first attention block, m randomly initialized embeddings were used as the query sequence to attend to the prompt’s hidden sequence, yielding a length m hidden sequence as the attention result; in the second attention block, the hidden sequence from the WaveNet layer was utilized as the query, and the attention result of length m served as the keys and values. The attention result of the second attention block was used as conditional information for the FilLM layer to perform affine transformations on the hidden sequence of the WaveNet in the diffusion model.</li>
</ul>
<h2 id="Connection-to-NaturalSpeech"><a href="#Connection-to-NaturalSpeech" class="headerlink" title="Connection to NaturalSpeech"></a>Connection to NaturalSpeech</h2><p>NaturalSpeech focuses on speech quality, producing sound comparable to human recordings. It only deals with single-speaker recorded data sets, such as LJSpeech. NaturalSpeech 2, on the other hand, emphasizes speech diversity.</p>
<p>NaturalSpeech 2 retains the fundamental components of NaturalSpeech, such as the encoder and decoder for waveform reconstruction, as well as previous modules (phoneme encoder, duration&#x2F;pitch predictor). However, it leverages:</p>
<ul>
<li>A diffusion model to increase modeling capability, capturing the complex and diverse data distribution in large-scale speech datasets.</li>
<li>Residual vector quantizers to regularize latent vectors, balancing reconstruction quality and prediction difficulty.</li>
<li>A speech prompt mechanism to achieve zero-shot capabilities not covered in a single-speaker synthesis system.</li>
</ul>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><ul>
<li><p>Audio features can be represented in both continuous and discrete forms. While discrete representations are more common, continuous features offer better performance. Therefore, this paper approximates continuous features through multiple discretizations.<br>Continuous vectors have lower compression rates and higher bit rates compared to discrete tokens, ensuring high-quality audio reconstruction.<br>Each audio frame is represented by a single vector, rather than multiple tokens as in discrete quantization, which does not increase the length of the hidden sequence.</p>
</li>
<li><p>Autoregressive models are sensitive to sequence length and error propagation, leading to unstable rhythm and robustness issues (e.g., word skipping, repetition, and folding). Considering the strict monotonic alignment and strong source-target dependencies in text-to-speech, using enhanced duration prediction and length-expanded diffusion models, this approach avoids robustness problems.</p>
</li>
<li><p>Due to the utilization of the diffusion model, the generation speed is slow.</p>
</li>
</ul>

      </section>
      <section class="extra">
        
          
        
        
        
        
  <nav class="nav">
    <a href="/2023/08/29/Paper%20Reading-Audio-Visual%20Efficient%20Conformer%20for%20Robust%20Speech%20Recognition/"><i class="iconfont iconleft"></i>Paper Reading-Audio-Visual Efficient Conformer for Robust Speech Recognition</a>
    <a href="/2023/04/26/Let%20them%20come%20to%20Berlin/">Let them come to Berlin<i class="iconfont iconright"></i></a>
  </nav>

      </section>
      
    </section>
  </div>
</article></div>
      <div class="col-xl-3">
        
          
  <aside class="toc-wrap">
    <h3 class="toc-title">Index</h3>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Background"><span class="toc-text">Background</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#TTS-for-Natural-Voice-Quality-and-Diversity"><span class="toc-text">TTS for Natural Voice: Quality and Diversity</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Neural-Audio-Codec"><span class="toc-text">Neural Audio Codec</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#NaturalSpeech-2"><span class="toc-text">NaturalSpeech 2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Neural-Audio-Codec-with-Continuous-Vectors"><span class="toc-text">Neural Audio Codec with Continuous Vectors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Latent-Diffusion-Model-with-Non-Autoregressive-Generation"><span class="toc-text">Latent Diffusion Model with Non-Autoregressive Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Speech-Prompting-for-In-Context-Learning"><span class="toc-text">Speech Prompting for In-Context Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Connection-to-NaturalSpeech"><span class="toc-text">Connection to NaturalSpeech</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Summary"><span class="toc-text">Summary</span></a></li></ol>
  </aside>

        
      </div>
    </div>
  </div>
</main>
  

  <footer class="footer">
    <div class="footer-social"><a href="mailto:Avidya@ieee.org " target="_blank"
  class="footer-social-item" onMouseOver="this.style.color=#FF3B00"
  onMouseOut="this.style.color='#33333D'">
  <i class="iconfont  iconmail"></i>
  </a></div>
    
  </footer>
  
      <div class="fab fab-plus">
    <i class="iconfont iconplus"></i>
  </div>
  
  
  
  <div class="fab fab-up">
    <i class="iconfont iconcaret-up"></i>
  </div>
  
  
  
    
<script src="/js/color-mode.js"></script>

  
  
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>





  
<script src="https://cdn.bootcdn.net/ajax/libs/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>




  
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>








<script src="/js/utils.js"></script>
<script src="/js/script.js"></script>







  <script>
    (function () {
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>












</html>