

<!DOCTYPE html>
<html lang="en" color-mode=light>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Overview of AI Agent - Avidya</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="google" content="notranslate" />
  
  <meta name="description" content="What is AI AgentWhile LLMs ...">
  <meta name="author" content="Kou">
  <link rel="icon" href="/images/icons/favicon-32x32.png" type="image/png" sizes="16x16">
  <link rel="icon" href="/images/icons/favicon-64x64.png" type="image/png" sizes="32x32">
  <link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180">
  <meta rel="mask-icon" href="/images/icons/stun-logo.svg" color="#333333">
  
    <meta rel="msapplication-TileImage" content="/images/icons/favicon-144x144.png">
    <meta rel="msapplication-TileColor" content="#000000">
  

  
<link rel="stylesheet" href="/css/style.css">


  
    
<link rel="stylesheet" href="https://at.alicdn.com/t/font_1445822_p6ry5n7lrr.css">

  

  
    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css">

  

  
    
      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/foundation.min.css" name="highlight-style" mode="light">

      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/solarized-dark.min.css" name="highlight-style" mode="dark">

      
  

  <script>
    var CONFIG = window.CONFIG || {};
    var ZHAOO = window.ZHAOO || {};
    CONFIG = {
      isHome: false,
      fancybox: true,
      pjax: false,
      loading: {
        gif: '/images/theme/loading.gif',
        lottie: ''
      },
      lazyload: {
        enable: true,
        only_post: 'false',
        loading: {
          gif: '/images/theme/loading.gif',
          lottie: ''
        }
      },
      donate: {
        enable: false,
        alipay: '',
        wechat: ''
      },
      galleries: {
        enable: false
      },
      fab: {
        enable: true,
        always_show: false
      },
      carrier: {
        enable: true
      },
      daovoice: {
        enable: false
      },
      preview: {
        background: {
          default: '',
          api: ''
        },
        motto: {
          default: 'もしも僕が主人公なら、僕は人の心が見えて',
          typing: true,
          api: '',
          data_contents: '["data","content"]'
        },
      },
      qrcode: {
        enable: false,
        type: 'url',
        image: '/images/theme/qr-code.png',
      },
      toc: {
        enable: true
      },
      scrollbar: {
        type: 'default'
      },
      notification: {
        enable: false,
        delay: 4500,
        list: '',
        page_white_list: '',
        page_black_list: ''
      },
      search: {
        enable: false,
        path: ''
      }
    }
  </script>

  

  

<meta name="generator" content="Hexo 6.3.0"></head>

<body class="lock-screen">
  <div class="loading" id="loading"></div>
  
    


  <nav class="navbar">
    <div class="left">
      
        <i class="iconfont iconhome j-navbar-back-home"></i>
      
      
      
        <i class="iconfont iconmoono" id="color-toggle" color-toggle="light"></i>
      
      
    </div>
    <div class="center">Overview of AI Agent</div>
    <div class="right">
      <i class="iconfont iconmenu j-navbar-menu"></i>
    </div>
    
  </nav>

  
  

<nav class="menu">
  <div class="menu-container">
    <div class="menu-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <ul class="menu-content"><li class="menu-item">
        <a href="/ " class="underline "> home</a>
      </li><li class="menu-item">
        <a href="/archives/ " class="underline "> archives</a>
      </li><li class="menu-item">
        <a href="/about/ " class="underline "> about</a>
      </li></ul>
    
  </div>
</nav>
  <main id="main">
  <div class="article-wrap">
    <div class="row container">
      <div class="col-xl-3"></div>
      <div class="col-xl-6"><article class="article">
  <div class="wrap">
    <section class="head">
  <img   class="lazyload" data-original="/images%5Ctheme%5CBraininvat.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="  draggable="false">
  <div class="head-mask">
    <h1 class="head-title">Overview of AI Agent</h1>
    <div class="head-info">
      <span class="post-info-item"><i class="iconfont iconcalendar"></i>December 30, 2023</span>
      
      <span class="post-info-item"><i class="iconfont iconfont-size"></i>11721</span>
    </div>
  </div>
</section>
    <section class="main">
      <section class="content">
        
        <h1 id="What-is-AI-Agent"><a href="#What-is-AI-Agent" class="headerlink" title="What is AI Agent"></a>What is AI Agent</h1><p>While LLMs have shown remarkable performance in generative tasks, they come with several drawbacks such as hallucinations, not always producing truthful results, outdated knowledge, and difficulty in dealing with complex computations. This led to the development of AI Agents, which can be seen as artificial intelligence brains that utilize LLMs for reasoning, planning, and taking actions, overcoming these limitations using external tools.<br>The concept of an Agent originates from philosophy, describing an entity with desires, beliefs, intentions, and the ability to act. In the field of artificial intelligence, this term has been given a new meaning, referring to intelligent entities with characteristics of autonomy, reactivity, proactiveness, and social ability. AI Agents are typically designed to have autonomous thinking and action capabilities. Users only need to provide a goal task, and the AI Agent will start working, aiming to achieve the goal in the best possible way based on its prior knowledge and environment.<br>The article <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.03442">“Generative Agents: Interactive Simulacra of Human Behavior”</a> from Stanford University delves into aspects such as memory, reasoning, and planning of AI Agents.<br><img   class="lazyload" data-original="/images%5Ctheme%5C45632948.png" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" ><br>The paper <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.07864">“The Rise and Potential of Large Language Model Based Agents: A Survey”</a> suggests that the development path from NLP to AGI can be divided into five levels: corpus, internet, perception, embodiment, and social attributes. Current LLMs have reached the second level, with text input and output at internet scale. With the addition of perception and action spaces, LLMs may reach the third and fourth levels. Furthermore, interaction and cooperation among multiple AI Agents to solve more complex problems or simulate real-world social behaviors could lead to the attainment of the fifth level. Thus, AI Agents can be seen as a pathway toward AGI to some extent.</p>
<h1 id="Framework-of-Intelligent-Agents"><a href="#Framework-of-Intelligent-Agents" class="headerlink" title="Framework of Intelligent Agents"></a>Framework of Intelligent Agents</h1><h2 id="Control-End-Brain"><a href="#Control-End-Brain" class="headerlink" title="Control End: Brain"></a>Control End: Brain</h2><p>It serves as the core of intelligent agents, responsible for storing memory and knowledge, as well as essential functions like information processing and decision-making. It can demonstrate the process of reasoning and planning, effectively handling unknown tasks, reflecting the generality and transferability of intelligent agents.</p>
<p>Natural Language Interaction: Language serves as a medium of communication containing rich information. Thanks to the powerful natural language generation and understanding capabilities of LLMs, intelligent agents can engage in multi-turn interactions with the external world through natural language, thereby achieving goals. Specifically, it can be divided into two aspects:</p>
<p>High-Quality Text Generation: Numerous evaluation experiments have shown that LLMs can generate fluent, diverse, novel, and controllable text. Although they may perform inadequately in individual languages, they generally possess good multilingual capabilities.</p>
<p>Understanding Implications: Apart from explicit content, language may convey intentions, preferences, and other information behind the speaker’s words. Understanding implications helps agents communicate and collaborate more efficiently, and large models have shown potential in this regard.</p>
<p>Knowledge: LLMs trained on large-scale corpora have the ability to store massive knowledge. In addition to linguistic knowledge, common sense knowledge and domain-specific knowledge are essential components of LLM-based Agents.</p>
<p>Although LLMs themselves still suffer from issues like knowledge obsolescence and hallucinations, some existing research mitigates these to some extent through knowledge editing or accessing external knowledge bases.</p>
<p>Memory: In this framework, the memory module stores the agent’s past observations, thoughts, and action sequences. Through specific memory mechanisms, the agent can effectively reflect on and apply previous strategies, enabling it to leverage past experiences to adapt to unfamiliar environments.</p>
<p>Short-Term Memory: I believe all contextual learning (see prompting engineering) utilizes the model’s short-term memory for learning.</p>
<p>Long-Term Memory: This provides the ability for the agent to retain and recall (potentially infinite) information over a long period, typically achieved through utilizing external vector storage and fast retrieval.</p>
<p>There are three main methods used to enhance memory capabilities:</p>
<p>Extend Backbone Architecture’s Length Limit: Improvements are made to address the inherent sequence length limitations of Transformers.</p>
<p>Summarizing Memory: Summarizing memory aids the agent in extracting key details from memory.</p>
<p>Compressing Memory: Memory compression using vectors or appropriate data structures can improve memory retrieval efficiency.</p>
<p>Furthermore, the method of memory retrieval is crucial; only by retrieving appropriate content can the agent access the most relevant and accurate information.</p>
<p>Reasoning &amp; Planning: Reasoning ability is crucial for intelligent agents to perform complex tasks such as decision-making and analysis. Specifically for LLMs, this involves methods like Chain-of-Thought (CoT) prompting. Planning is a commonly used strategy when facing large challenges. It helps the agent organize thoughts, set goals, and determine steps to achieve those goals. In implementation, planning can include two steps:</p>
<p>Plan Formulation: The agent decomposes complex tasks into more manageable sub-tasks. For example: decomposing and executing in sequence, step-by-step planning and execution, or multi-path planning and selecting the optimal path. In scenarios requiring domain expertise, the agent can integrate with domain-specific Planner modules to enhance capabilities.</p>
<p>Plan Reflection: After formulating a plan, it can be reflected upon and evaluated for its quality. This reflection can come from three sources: internal feedback mechanisms, interaction with humans for feedback, and feedback from the environment.</p>
<p>Transferability &amp; Generality: LLMs equipped with world knowledge provide intelligent agents with powerful transferability and generality. A good agent is not just a static knowledge base but also possesses dynamic learning capabilities:</p>
<p>Generalization to Unknown Tasks: With increasing model size and training data, LLMs have shown remarkable capabilities in solving unknown tasks. Large models fine-tuned with instructions perform well in zero-shot testing, achieving results comparable to expert models in many tasks.</p>
<p>In-context Learning: Large models not only learn from a small number of contextual examples but also extend this ability to out-of-text multimodal scenarios, providing more possibilities for the application of agents in the real world.</p>
<p>Continual Learning: The main challenge of continual learning is catastrophic forgetting, where the model tends to lose knowledge from past tasks while learning new ones. Domain-specific intelligent agents should strive to avoid forgetting knowledge in general domains.</p>
<h2 id="Perception-End-Perception"><a href="#Perception-End-Perception" class="headerlink" title="Perception End: Perception"></a>Perception End: Perception</h2><p>Multimodal perception deepens the agent’s understanding of the working environment, significantly enhancing its versatility.</p>
<p>Text Input: As the most basic capability of LLMs, it will not be elaborated here.</p>
<p>Visual Input: LLMs themselves do not possess visual perception capabilities but can understand discrete textual content. However, visual input typically contains a wealth of information about the world, including object properties, spatial relationships, scene layouts, etc. Common methods include:</p>
<p>Converting visual input into corresponding textual descriptions (Image Captioning): This can be directly understood by LLMs and offers high interpretability.</p>
<p>Encoding visual information: A paradigm of visual base model + LLMs is used to construct perception modules, with alignment operations allowing the model to understand different modalities of content, trained in an end-to-end manner.</p>
<p>Auditory Input: Audition is also an important part of human perception. With LLMs’ excellent tool invocation capabilities, an intuitive idea is that the agent can act as a control hub, perceiving audio information by cascading existing toolsets or expert models. Additionally, audio can be represented visually through spectrograms. Spectrograms can present 2D information as plane images, allowing some visual processing methods to be transferred to the audio domain.</p>
<p>Other Inputs: Real-world information is not limited to text, vision, and audition. Authors hope that in the future, intelligent agents will be equipped with richer perception modules, such as tactile and olfactory organs, for acquiring richer attributes of target objects. Meanwhile, agents should have a clear perception of the surrounding environment’s temperature, humidity, and brightness, enabling more environment-aware actions.</p>
<p>Furthermore, agents can be introduced to perceive a broader overall environment, using mature perception modules such as LiDAR, GPS, inertial measurement units, etc.</p>
<h2 id="Action-End-Action"><a href="#Action-End-Action" class="headerlink" title="Action End: Action"></a>Action End: Action</h2><p>After analyzing and decision-making in the brain, the agent needs to take actions to adapt to or change the environment.</p>
<p>Text Output: As the most basic capability of LLMs, it will not be elaborated here.</p>
<p>Tool Usage: Despite having excellent knowledge reserves and professional capabilities, LLMs may face challenges in robustness and hallucinations when dealing with specific problems. Meanwhile, tools, as extensions of user capabilities, can provide assistance in professionalism, factualness, and interpretability. For example, a calculator can be used to calculate mathematical problems, and a search engine can be used to search for real-time information.</p>
<p>Additionally, tools can expand the action space of intelligent agents. For example, by invoking expert models for tasks like speech generation and image generation, agents can obtain multimodal ways of action. Therefore, learning how to be proficient tool users, i.e., learning how to effectively utilize tools, is an important and promising direction.</p>
<p>Currently, the main methods of tool learning include learning from demonstrations and learning from feedback. Additionally, agents can acquire generalization capabilities in using various tools through meta-learning, curriculum learning, etc. Furthermore, intelligent agents can further learn how to “self-sufficiently” manufacture tools, thereby enhancing their autonomy and independence.</p>
<p>Embodied Action: Embodiment refers to the ability of agents to understand, modify the environment, and update their own states while interacting with the environment. Embodied Action is considered a bridge between virtual intelligence and physical reality.</p>
<p>Traditional reinforcement learning-based agents have limitations in sample efficiency, generalization, and complex problem reasoning, while LLM-based Agents, by introducing rich inherent knowledge of large models, enable Embodied Agents to actively perceive, influence the physical environment like humans. Depending on the agent’s degree of autonomy in tasks or the complexity of actions, there can be the following atomic actions:</p>
<p>Observation: Helps intelligent agents locate their positions in the environment, perceive object properties, and obtain other environmental information.</p>
<p>Manipulation: Involves completing specific manipulation tasks such as grabbing, pushing, etc.</p>
<p>Navigation: Requires intelligent agents to change their positions according to task goals and update their own states based on environmental information.</p>
<p>By combining these atomic actions, the agent can accomplish more complex tasks. For example, in a embodied QA task like “Is the watermelon in the kitchen larger than the bowl?”, the agent needs to navigate to the kitchen and determine the relative sizes of the two objects.</p>

      </section>
      <section class="extra">
        
          
        
        
        
        
  <nav class="nav">
    <a href="/2024/01/08/chinese%20beaver/"><i class="iconfont iconleft"></i>Chinese Beaver</a>
    <a href="/2023/11/25/Bert-VITS2OpenVoiceVITS2/">Bert-VITS2<i class="iconfont iconright"></i></a>
  </nav>

      </section>
      
    </section>
  </div>
</article></div>
      <div class="col-xl-3">
        
          
  <aside class="toc-wrap">
    <h3 class="toc-title">Index</h3>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#What-is-AI-Agent"><span class="toc-text">What is AI Agent</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Framework-of-Intelligent-Agents"><span class="toc-text">Framework of Intelligent Agents</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Control-End-Brain"><span class="toc-text">Control End: Brain</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Perception-End-Perception"><span class="toc-text">Perception End: Perception</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Action-End-Action"><span class="toc-text">Action End: Action</span></a></li></ol></li></ol>
  </aside>

        
      </div>
    </div>
  </div>
</main>
  

  <footer class="footer">
    <div class="footer-social"><a href="mailto:Avidya@ieee.org " target="_blank"
  class="footer-social-item" onMouseOver="this.style.color=#FF3B00"
  onMouseOut="this.style.color='#33333D'">
  <i class="iconfont  iconmail"></i>
  </a></div>
    
  </footer>
  
      <div class="fab fab-plus">
    <i class="iconfont iconplus"></i>
  </div>
  
  
  
  <div class="fab fab-up">
    <i class="iconfont iconcaret-up"></i>
  </div>
  
  
  
    
<script src="/js/color-mode.js"></script>

  
  
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>





  
<script src="https://cdn.bootcdn.net/ajax/libs/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>




  
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>








<script src="/js/utils.js"></script>
<script src="/js/script.js"></script>







  <script>
    (function () {
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>












</html>