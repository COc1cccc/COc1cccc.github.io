

<!DOCTYPE html>
<html lang="en" color-mode=light>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Paper Reading-Variational Inference with adversarial learning for end-to-end Text-to-Speech - Avidya</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="google" content="notranslate" />
  
  <meta name="description" content="Variational Inference with ...">
  <meta name="author" content="Kou">
  <link rel="icon" href="/images/icons/favicon-32x32.png" type="image/png" sizes="16x16">
  <link rel="icon" href="/images/icons/favicon-64x64.png" type="image/png" sizes="32x32">
  <link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180">
  <meta rel="mask-icon" href="/images/icons/stun-logo.svg" color="#333333">
  
    <meta rel="msapplication-TileImage" content="/images/icons/favicon-144x144.png">
    <meta rel="msapplication-TileColor" content="#000000">
  

  
<link rel="stylesheet" href="/css/style.css">


  
    
<link rel="stylesheet" href="https://at.alicdn.com/t/font_1445822_p6ry5n7lrr.css">

  

  
    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css">

  

  
    
      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/foundation.min.css" name="highlight-style" mode="light">

      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/solarized-dark.min.css" name="highlight-style" mode="dark">

      
  

  <script>
    var CONFIG = window.CONFIG || {};
    var ZHAOO = window.ZHAOO || {};
    CONFIG = {
      isHome: false,
      fancybox: true,
      pjax: false,
      loading: {
        gif: '/images/theme/loading.gif',
        lottie: ''
      },
      lazyload: {
        enable: true,
        only_post: 'false',
        loading: {
          gif: '/images/theme/loading.gif',
          lottie: ''
        }
      },
      donate: {
        enable: false,
        alipay: '',
        wechat: ''
      },
      galleries: {
        enable: false
      },
      fab: {
        enable: true,
        always_show: false
      },
      carrier: {
        enable: true
      },
      daovoice: {
        enable: false
      },
      preview: {
        background: {
          default: '',
          api: ''
        },
        motto: {
          default: 'もしも僕が主人公なら、僕は人の心が見えて',
          typing: true,
          api: '',
          data_contents: '["data","content"]'
        },
      },
      qrcode: {
        enable: false,
        type: 'url',
        image: '/images/theme/qr-code.png',
      },
      toc: {
        enable: true
      },
      scrollbar: {
        type: 'default'
      },
      notification: {
        enable: false,
        delay: 4500,
        list: '',
        page_white_list: '',
        page_black_list: ''
      },
      search: {
        enable: false,
        path: ''
      }
    }
  </script>

  

  

<meta name="generator" content="Hexo 6.3.0"></head>

<body class="lock-screen">
  <div class="loading" id="loading"></div>
  
    


  <nav class="navbar">
    <div class="left">
      
        <i class="iconfont iconhome j-navbar-back-home"></i>
      
      
      
        <i class="iconfont iconmoono" id="color-toggle" color-toggle="light"></i>
      
      
    </div>
    <div class="center">Paper Reading-Variational Inference with adversarial learning for end-to-end Text-to-Speech</div>
    <div class="right">
      <i class="iconfont iconmenu j-navbar-menu"></i>
    </div>
    
  </nav>

  
  

<nav class="menu">
  <div class="menu-container">
    <div class="menu-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <ul class="menu-content"><li class="menu-item">
        <a href="/ " class="underline "> home</a>
      </li><li class="menu-item">
        <a href="/archives/ " class="underline "> archives</a>
      </li><li class="menu-item">
        <a href="/about/ " class="underline "> about</a>
      </li></ul>
    
  </div>
</nav>
  <main id="main">
  <div class="article-wrap">
    <div class="row container">
      <div class="col-xl-3"></div>
      <div class="col-xl-6"><article class="article">
  <div class="wrap">
    <section class="head">
  <img   class="lazyload" data-original="/images/theme/post-image.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="  draggable="false">
  <div class="head-mask">
    <h1 class="head-title">Paper Reading-Variational Inference with adversarial learning for end-to-end Text-to-Speech</h1>
    <div class="head-info">
      <span class="post-info-item"><i class="iconfont iconcalendar"></i>November 23, 2022</span>
      
      <span class="post-info-item"><i class="iconfont iconfont-size"></i>8260</span>
    </div>
  </div>
</section>
    <section class="main">
      <section class="content">
        
        <p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/419883319">Variational Inference with adversarial learning for end-to-end Text-to-Speech</a></p>
<p>VITS (Variational Inference with adversarial learning for end-to-end Text-to-Speech) is a highly expressive speech synthesis model that combines variational inference, normalizing flows, and adversarial training. Unlike traditional methods that directly concatenate spectral features in speech synthesis, VITS connects the acoustic model and the vocoder using latent variables. It models randomness on the latent variables and utilizes a stochastic duration predictor to enhance the diversity of synthesized speech. With VITS, given the same input text, it can generate speech with different tones and rhythms, providing greater flexibility and naturalness in speech synthesis.</p>
<h1 id="Proposed-Approach"><a href="#Proposed-Approach" class="headerlink" title="Proposed Approach"></a>Proposed Approach</h1><p>VITS mainly consists of three components:</p>
<p>Conditional Variational AutoEncoder (VAE)<br>Alignment estimates generated from variational inference<br>Generative adversarial training</p>
<h1 id="Variational-Inference"><a href="#Variational-Inference" class="headerlink" title="Variational Inference"></a>Variational Inference</h1><p>The generator of VITS can be viewed as maximizing the variational lower bound, also known as the Evidence Lower Bound (ELBO), of a conditional Variational AutoEncoder (VAE).</p>
<ol>
<li>Reconstruction Loss<br>During training, VITS still generates Mel-spectrograms to guide the model training. The target sample points in the reconstruction loss are Mel-spectrograms rather than the original waveform:</li>
</ol>
<p>$$<br>L_{recon}&#x3D;||x_{mel}-\hat{x}_{mel}||_1<br>$$</p>
<p>However, during inference, there is no need to generate Mel-spectrograms. Mel-spectrograms are only used to compute this reconstruction loss.</p>
<ol start="2">
<li>KL Divergence<br>The input to the prior encoder $c$ includes phonemes generated from the text $c_{text}$ and the alignment between phonemes and latent variables $A$. The alignment matrix $A$ is of size $|c_{text}|\times |z|$, representing the strictly monotonic attention matrix for each phoneme’s duration. Therefore, the KL divergence is:</li>
</ol>
<p>$$<br>L_{kl}&#x3D;log\ q_{\phi}(z|x_{lin})-log\ p_{\theta}(z|c_{text},A)<br>$$</p>
<p>where $q_{\phi}(z|x)$ represents the posterior distribution of outputting latent variables $z$ given the linear spectrogram $x$, and $p_{\theta}(z|c)$ represents the prior distribution of outputting latent variables $z$ given the condition $c$. The latent variable $z$ is sampled from:</p>
<p>$$<br>z\sim q_{\phi}(z|x_{lin})&#x3D;N(z;\mu_{\phi}(x_{lin}),\sigma_{\phi}(x_{lin}))<br>$$</p>
<p>To provide higher resolution information to the posterior encoder, linear spectrograms rather than Mel-spectrograms are used as the input to the posterior encoder $q_{\phi}$. Additionally, to generate more realistic samples, it’s crucial to enhance the expressive power of the prior distribution. Therefore, normalizing flows are introduced to enable reversible transformations between the simple distribution generated by the text encoder and the complex distribution corresponding to latent variables $z$. After the up-sampled encoder output, a series of reversible transformations are applied:<br>$$<br>p_{\theta}(z|c)&#x3D;N(f_{\theta}(z);\mu_{\theta}(c),\sigma_{\theta}(c))|det\frac{\partial f_{\theta}(z)}{\partial z} |<br>$$</p>
<p>Here, the input $c$ is the up-sampled encoder output:</p>
<p>$$<br>c&#x3D;[c_{text},A]<br>$$</p>
<h2 id="Alignment-Estimation"><a href="#Alignment-Estimation" class="headerlink" title="Alignment Estimation"></a>Alignment Estimation</h2><p>Since there are no real “alignment” labels during training, it is necessary to estimate the alignment between text and audio at each iteration during training.</p>
<ol>
<li>Monotonic Alignment Search<br>To estimate the alignment $A$ between text and speech, VITS employs a method similar to Monotonic Alignment Search (MAS) used in Glow-TTS. This method aims to find an optimal alignment path to maximize the log-likelihood of the data parameterized by the normalization flow $f$:</li>
</ol>
<p>$$<br>A&#x3D;\mathop{argmax}\limits{\hat{A}}\mathop{log}p(x|c_{text},\hat{A})\ &#x3D;\mathop{argmax}\limits{\hat{A}}\mathop{log}N(f(x);\mu(c_{text},\hat{A}),\sigma(c_{text},\hat{A}))<br>$$</p>
<p>MAS Constraints: The optimal alignment obtained through MAS must be monotonic and non-skipping. However, directly applying MAS to VITS is not feasible because VITS optimizes the ELBO rather than the log-likelihood of deterministic latent variables $z$. Therefore, a slight modification to MAS is made to find the optimal alignment path that maximizes the ELBO:<br>$$<br>\mathop{argmax}\limits{\hat{A}}\mathop{log}p_{\theta}(x_{mel}|z)-\mathop{log}\frac{q_\phi(z|x_{lin})}{p_\theta(z|c_{text},\hat{A})}&#x3D;\mathop{argmax}\limits{\hat{A}}\mathop{log}p_\theta(z|c_{text},\hat{A})&#x3D;\mathop{log}N(f_{\theta}(z);\mu_{\theta}(c_{text},\hat{A}),\sigma_{\theta}(c_{text},\hat{A}))<br>$$</p>
<p>2.<br>Predicting Duration from Text</p>
<p>The stochastic duration predictor is a flow-based generative model that introduces random variables $u$ and $v$ with the same time resolution and dimensionality as the duration sequence. It samples these two variables using the approximate posterior distribution $q_{\phi}(u,v|d,c_{text})$. The training objective is the variational lower bound of the phoneme duration log-likelihood:<br>$$<br>\mathop{log}p\theta(d|c_{text})\geq \mathbb{E}<em>{q</em>{\phi}(u,v|d,c_{text})}[\mathop{log}\frac{p_{\theta}(d-u,v|c_{text})}{q_{\phi}(u,v|d,c_{text})}]<br>$$</p>
<p>During training, the gradients of the stochastic duration predictor are disconnected to prevent them from affecting other modules. Phoneme durations are sampled from random noise through the reversible transformation of the stochastic duration predictor and then converted into integer values.</p>
<h2 id="Adversarial-Training"><a href="#Adversarial-Training" class="headerlink" title="Adversarial Training"></a>Adversarial Training</h2><p>The discriminator $D$ is introduced to distinguish whether the output is generated by the decoder $G$ or real waveform $y$. VITS employs two types of loss functions. The first one is the least-squares loss function used for adversarial training:<br>$$<br>L_{adv}(D)&#x3D;\mathbb{E}_{(y,z)}[(D(y)-1)^2+(D(G(z)))^2]<br>$$</p>
<p>$$<br>L_{adv}(G)&#x3D;\mathbb{E}_z[(D(G(z))-1)^2]<br>$$</p>
<p>The second type is the feature-matching loss, specifically applied to the generator:</p>
<p>$$<br>L_{fm}(G)&#x3D;\mathbb{E}<em>{(y,z)}[\sum</em>{l&#x3D;1}^{T}\frac{1}{N_l}||D^l(y)-D^l(G(z))||_1]<br>$$</p>
<p>Where $T$ represents the number of layers in the discriminator, $D^l$ denotes the output feature map of the $l$-th layer of the discriminator, and $N_l$ represents the number of feature maps. The feature-matching loss can be considered as a reconstruction loss, used to constrain the output of intermediate layers in the discriminator.</p>
<h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><p>VITS can be seen as a joint training of VAE and GAN. Therefore, the overall loss is given by:</p>
<p>$$<br>L_{vae}&#x3D;L_{recon}+L_{kl}+L_{dur}+L_{adv}+L_{fm}(G)<br>$$</p>
<h2 id="Structure"><a href="#Structure" class="headerlink" title="Structure"></a>Structure</h2><p>The overall structure of VITS can be divided into 5 components:</p>
<p>Prior Encoder: Text Encoder + Normalizing Flow $f_{\theta}$ to enhance the complexity of the prior distribution. When applied to multi-speaker models, speaker embedding vectors are added to the residual modules of the normalizing flow.</p>
<p>Decoder: Essentially the generator of the vocoder HiFi-GAN V1. When applied to multi-speaker models, a linear layer is added after the speaker embedding vectors, which are then concatenated to the output latent variables $z$ of $f_{\theta}$.</p>
<p>Stochastic Duration Predictor: Estimates the distribution of phoneme durations from conditional input $h_{text}$. When applied to multi-speaker models, a linear layer is added after the speaker embedding vectors, which are then concatenated to the output $h_{text}$ of the text encoder.</p>
<p>Posterior Encoder: Takes linear spectrograms as input during training and outputs latent variables $z$, while during inference, latent variables $z$ are generated by $f_{\theta}$. The posterior encoder of VITS adopts the non-causal WaveNet residual modules used in WaveGlow and Glow-TTS. When applied to multi-speaker models, speaker embedding vectors are added to the residual modules. Used for training only.</p>
<p>Discriminator: Essentially the multi-scale discriminator of HiFi-GAN. Used for training only.</p>
<h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>In summary, VITS is an end-to-end speech synthesis model that directly maps characters or phonemes to waveforms. The model adopts a generative adversarial training mode, with its generator consisting of multiple modules based on normalizing flows. VITS has a large model size and exhibits excellent synthesis quality.</p>
<p>While the idea behind VITS is insightful, understanding its workings can be challenging, especially regarding the normalizing flows component. For more information on normalizing flows, you can refer to<a target="_blank" rel="noopener" href="https://github.com/janosh/awesome-normalizing-flows">Awesome Normalizing Flows</a>。</p>

      </section>
      <section class="extra">
        
          
        
        
        
        
  <nav class="nav">
    <a href="/2022/12/12/snow/"><i class="iconfont iconleft"></i>❄️</a>
    <a href="/2022/10/07/stella/">🌟<i class="iconfont iconright"></i></a>
  </nav>

      </section>
      
    </section>
  </div>
</article></div>
      <div class="col-xl-3">
        
          
  <aside class="toc-wrap">
    <h3 class="toc-title">Index</h3>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Proposed-Approach"><span class="toc-text">Proposed Approach</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Variational-Inference"><span class="toc-text">Variational Inference</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Alignment-Estimation"><span class="toc-text">Alignment Estimation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adversarial-Training"><span class="toc-text">Adversarial Training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Loss"><span class="toc-text">Loss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Structure"><span class="toc-text">Structure</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Summary"><span class="toc-text">Summary</span></a></li></ol>
  </aside>

        
      </div>
    </div>
  </div>
</main>
  

  <footer class="footer">
    <div class="footer-social"><a href="mailto:Avidya@ieee.org " target="_blank"
  class="footer-social-item" onMouseOver="this.style.color=#FF3B00"
  onMouseOut="this.style.color='#33333D'">
  <i class="iconfont  iconmail"></i>
  </a></div>
    
  </footer>
  
      <div class="fab fab-plus">
    <i class="iconfont iconplus"></i>
  </div>
  
  
  
  <div class="fab fab-up">
    <i class="iconfont iconcaret-up"></i>
  </div>
  
  
  
    
<script src="/js/color-mode.js"></script>

  
  
</body>

<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>





  
<script src="https://cdn.bootcdn.net/ajax/libs/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>




  
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>








<script src="/js/utils.js"></script>
<script src="/js/script.js"></script>







  <script>
    (function () {
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>












</html>