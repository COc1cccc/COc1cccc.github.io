

<!DOCTYPE html>
<html lang="en" color-mode=light>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Paper Reading-ZeRO  Memory optimizations toward training trillion parameter models - Avidya</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="google" content="notranslate" />
  
  <meta name="description" content="ZeRO: Memory optimizations ...">
  <meta name="author" content="Kou">
  <link rel="icon" href="/images/icons/favicon-32x32.png" type="image/png" sizes="16x16">
  <link rel="icon" href="/images/icons/favicon-64x64.png" type="image/png" sizes="32x32">
  <link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180">
  <meta rel="mask-icon" href="/images/icons/stun-logo.svg" color="#333333">
  
    <meta rel="msapplication-TileImage" content="/images/icons/favicon-144x144.png">
    <meta rel="msapplication-TileColor" content="#000000">
  

  
<link rel="stylesheet" href="/css/style.css">


  
    
<link rel="stylesheet" href="https://at.alicdn.com/t/font_1445822_p6ry5n7lrr.css">

  

  
    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css">

  

  
    
      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/foundation.min.css" name="highlight-style" mode="light">

      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/solarized-dark.min.css" name="highlight-style" mode="dark">

      
  

  <script>
    var CONFIG = window.CONFIG || {};
    var ZHAOO = window.ZHAOO || {};
    CONFIG = {
      isHome: false,
      fancybox: true,
      pjax: false,
      loading: {
        gif: '/images/theme/loading.gif',
        lottie: ''
      },
      lazyload: {
        enable: true,
        only_post: 'false',
        loading: {
          gif: '/images/theme/loading.gif',
          lottie: ''
        }
      },
      donate: {
        enable: false,
        alipay: '',
        wechat: ''
      },
      galleries: {
        enable: false
      },
      fab: {
        enable: true,
        always_show: false
      },
      carrier: {
        enable: true
      },
      daovoice: {
        enable: false
      },
      preview: {
        background: {
          default: '',
          api: ''
        },
        motto: {
          default: 'もしも僕が主人公なら、僕は人の心が見えて',
          typing: true,
          api: '',
          data_contents: '["data","content"]'
        },
      },
      qrcode: {
        enable: false,
        type: 'url',
        image: '/images/theme/qr-code.png',
      },
      toc: {
        enable: true
      },
      scrollbar: {
        type: 'default'
      },
      notification: {
        enable: false,
        delay: 4500,
        list: '',
        page_white_list: '',
        page_black_list: ''
      },
      search: {
        enable: false,
        path: ''
      }
    }
  </script>

  

  

<meta name="generator" content="Hexo 6.3.0"></head>

<body class="lock-screen">
  <div class="loading" id="loading"></div>
  
    


  <nav class="navbar">
    <div class="left">
      
        <i class="iconfont iconhome j-navbar-back-home"></i>
      
      
      
        <i class="iconfont iconmoono" id="color-toggle" color-toggle="light"></i>
      
      
    </div>
    <div class="center">Paper Reading-ZeRO  Memory optimizations toward training trillion parameter models</div>
    <div class="right">
      <i class="iconfont iconmenu j-navbar-menu"></i>
    </div>
    
  </nav>

  
  

<nav class="menu">
  <div class="menu-container">
    <div class="menu-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <ul class="menu-content"><li class="menu-item">
        <a href="/ " class="underline "> home</a>
      </li><li class="menu-item">
        <a href="/archives/ " class="underline "> archives</a>
      </li><li class="menu-item">
        <a href="/about/ " class="underline "> about</a>
      </li></ul>
    
  </div>
</nav>
  <main id="main">
  <div class="article-wrap">
    <div class="row container">
      <div class="col-xl-3"></div>
      <div class="col-xl-6"><article class="article">
  <div class="wrap">
    <section class="head">
  <img   class="lazyload" data-original="/images/theme/post-image.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="  draggable="false">
  <div class="head-mask">
    <h1 class="head-title">Paper Reading-ZeRO  Memory optimizations toward training trillion parameter models</h1>
    <div class="head-info">
      <span class="post-info-item"><i class="iconfont iconcalendar"></i>July 14, 2022</span>
      
      <span class="post-info-item"><i class="iconfont iconfont-size"></i>6898</span>
    </div>
  </div>
</section>
    <section class="main">
      <section class="content">
        
        <p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.02054">ZeRO: Memory optimizations toward training trillion parameter models</a></p>
<p>ZeRO is an open-source DL (Deep Learning) training optimization library within DeepSpeed, which offers significant optimization effects for accelerating training of large models. Recently, I utilized in fine-tuning tasks, prompting a desire to revisit the paper.</p>
<p>ZeRO’s focus lies in efficiently applying data parallelism to training massively scaled neural networks. We know that one advantage of data parallelism is the ability to perform gradient calculations with some synchronization, thereby concealing communication overheads. Another advantage is that it doesn’t require knowledge of the model’s architecture, unlike when partitioning models.</p>
<p>The core idea of ZeRO is that when a model becomes too large to fit into a single computing unit, including when it exceeds the memory capacity of CPUs or GPUs, the entire model is distributed across various locations. Then, during computation, the necessary portions of the model are retrieved from this distributed storage. Once these portions are used, they are discarded.</p>
<h1 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h1><p>The traditional DP (Data Parallelism) methods cannot reduce the memory usage per device, leading to the inability to train models with more than 14 billion parameters on GPUs with 32GB VRAM at that time.</p>
<p>Existing methods such as pipeline parallelism, model parallelism, and CPU offloading involve trade-offs in functionality, memory usage, and computational communication efficiency.</p>
<p>The author mentions that model parallelism shows the most potential. This method involves vertically partitioning the model and distributing the computation and parameters of each layer across multiple devices. At that time, models like the 11-billion-parameter T5 and the 8.3-billion-parameter Megatron-LM utilized model parallelism extensively. However, this approach requires substantial inter-layer communication. Although it performs well within a single node, the author tested a 40-billion-parameter model using Megatron-LM on two DGX-2 nodes, revealing that each V100 GPU’s computational performance was only 5 TFLOPS.</p>
<p>The author found that memory consumption mainly concentrates on:</p>
<ul>
<li>Model state: optimizer, gradients, parameters.</li>
<li>Residual states: activation values, buffers, memory fragments.</li>
</ul>
<p>To address these issues, the author proposes ZeRO (Zero Redundancy Optimizer).</p>
<h1 id="Optimizing-Model-State-Memory"><a href="#Optimizing-Model-State-Memory" class="headerlink" title="Optimizing Model State Memory:"></a>Optimizing Model State Memory:</h1><p>Data Parallelism (DP) drawbacks: While it doesn’t require frequent communication, it necessitates copying the entire model state in each process, resulting in memory wastage.</p>
<p>Model Parallelism (MP) drawbacks: While it efficiently utilizes memory by partitioning the model state, it leads to overly fine-grained computation, requiring frequent communication.</p>
<p>Overall, the above methods statically maintain all model states throughout the entire training process, even though not all model states are always needed during training.</p>
<p>Improvements:</p>
<p>ZeRO-DP is proposed, powered by ZeRO, which eliminates memory redundancy in DP by partitioning the model state instead of copying it. It utilizes dynamic communication scheduling strategies to optimize computational communication efficiency, introducing three optimization stages:</p>
<p>Optimizer partitioning (Pos): Reduces memory by 4 times with the same communication volume as DP.</p>
<p>Gradient partitioning (Pos+g): Reduces memory by 8 times with the same communication volume as DP.</p>
<p>Model parameter partitioning (Pos+g+p): The memory reduction is linearly proportional to the number of partitions in DP. For example, splitting across 64 GPUs will reduce memory by 64 times, with only a 50% increase in communication.</p>
<p>The author calculates that with these optimizations, training a trillion-parameter model on 1024 GPUs using fp16 would require 16TB of space to store optimizer states, gradients, and parameters, with each card occupying 16GB of VRAM.</p>
<h1 id="Optimizing-Residual-States-Memory"><a href="#Optimizing-Residual-States-Memory" class="headerlink" title="Optimizing Residual States Memory:"></a>Optimizing Residual States Memory:</h1><p>To optimize the memory usage of activation values, buffers, and memory fragments, ZeRO-R is introduced with several optimization points:</p>
<ul>
<li><p>Checkpointing for Activation Values: Utilizing checkpointing for activation values to save memory. Activation values are sliced, and as needed, the data is transferred to the CPU to optimize memory usage.</p>
</li>
<li><p>Proper Definition of Temporary Buffer Sizes: ZeRO-R defines appropriate temporary buffer sizes to balance memory and computational efficiency.</p>
</li>
<li><p>Active Memory Management Based on Tensor Lifecycles: Actively managing memory based on the lifecycle of different tensors to prevent memory fragmentation.</p>
</li>
</ul>
<p>In summary, ZeRO primarily consists of two optimizations: ZeRO-DP and ZeRO-R, which are combined to achieve efficient memory usage and computational performance.</p>
<h1 id="ZeRO-Combined-with-Model-Parallelism-MP"><a href="#ZeRO-Combined-with-Model-Parallelism-MP" class="headerlink" title="ZeRO Combined with Model Parallelism (MP):"></a>ZeRO Combined with Model Parallelism (MP):</h1><p>While the strategy of MP becomes less crucial after employing ZeRO’s policies, MP still requires modifications to the model and comes with various constraints compared to DP.</p>
<p>However, when activation memory usage becomes significant, ZeRO-R’s strategies may not fully optimize training. In such cases, combining MP can reduce activation memory usage. Moreover, theoretically, combining ZeRO and MP can optimize memory usage to Nd * Nm times (where Nd is the number of devices and Nm is the number of model partitions in MP).</p>
<p>In scenarios involving smaller models, solely relying on DP may lead to oversized batch sizes that could hinder convergence. In contrast, using MP can help accelerate training while reducing the batch size to an appropriate level, aiding in model convergence.</p>
<h1 id="Implementation-Evaluation"><a href="#Implementation-Evaluation" class="headerlink" title="Implementation &amp; Evaluation"></a>Implementation &amp; Evaluation</h1><ul>
<li><p>Model Size: By combining Megatron with MP, ZeRO-100B efficiently operates a model with 170 billion parameters, surpassing the scalability limitations of existing systems like Megatron alone, which struggle beyond the 40 billion parameter scale. Compared to the state-of-the-art (SOTA), the model size has increased by more than 8 times.</p>
</li>
<li><p>Training Speed: Improved memory efficiency enhances throughput and training speed. Running on 400 Nvidia V100 GPUs, ZeRO achieves a training speed of 1000 billion parameters at 38 TFlops per GPU, totaling over 15 Petaflops in performance. Compared to SOTA, for the same model size, training speed has increased by over 10 times.</p>
</li>
<li><p>Scalability: Performance exhibits superlinear acceleration when using 64-400 GPUs. This is a characteristic of ZeRO-DP, which reduces memory usage of model states as the degree of DP increases, enabling each GPU to handle larger batch sizes, thus improving performance.</p>
</li>
<li><p>Feasibility of Large-scale Training: ZeRO-100B enables training of a 13 billion parameter model with just refactoring. In contrast, existing systems like PyTorch Distributed Data Parallel encounter memory limitations even at the 1.4 billion parameter scale.</p>
</li>
<li><p>SOTA: ZeRO supports the Turing-NLG model with 17 billion parameters, achieving state-of-the-art results.</p>
</li>
</ul>

      </section>
      <section class="extra">
        
          
        
        
        
        
  <nav class="nav">
    <a href="/2022/08/06/127.%20Word%20Ladder/"><i class="iconfont iconleft"></i>Solution --- 127. Word Ladder</a>
    <a href="/2022/06/14/403.%20Frog%20Jump/">Solution --- 403. Frog Jump<i class="iconfont iconright"></i></a>
  </nav>

      </section>
      
    </section>
  </div>
</article></div>
      <div class="col-xl-3">
        
          
  <aside class="toc-wrap">
    <h3 class="toc-title">Index</h3>
    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Background"><span class="toc-text">Background</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Optimizing-Model-State-Memory"><span class="toc-text">Optimizing Model State Memory:</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Optimizing-Residual-States-Memory"><span class="toc-text">Optimizing Residual States Memory:</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ZeRO-Combined-with-Model-Parallelism-MP"><span class="toc-text">ZeRO Combined with Model Parallelism (MP):</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Implementation-Evaluation"><span class="toc-text">Implementation &amp; Evaluation</span></a></li></ol>
  </aside>

        
      </div>
    </div>
  </div>
</main>
  

  <footer class="footer">
    <div class="footer-social"><a href="mailto:Avidya@ieee.org " target="_blank"
  class="footer-social-item" onMouseOver="this.style.color=#FF3B00"
  onMouseOut="this.style.color='#33333D'">
  <i class="iconfont  iconmail"></i>
  </a></div>
    
  </footer>
  
      <div class="fab fab-plus">
    <i class="iconfont iconplus"></i>
  </div>
  
  
  
  <div class="fab fab-up">
    <i class="iconfont iconcaret-up"></i>
  </div>
  
  
  
    
<script src="/js/color-mode.js"></script>

  
  
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>





  
<script src="https://cdn.bootcdn.net/ajax/libs/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>




  
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>








<script src="/js/utils.js"></script>
<script src="/js/script.js"></script>







  <script>
    (function () {
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>












</html>