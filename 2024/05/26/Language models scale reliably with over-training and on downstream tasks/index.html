

<!DOCTYPE html>
<html lang="en" color-mode=light>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>Language models scale reliably with over-training and on downstream tasks - Avidya</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="google" content="notranslate" />
  
  <meta name="description" content="Language models scale relia...">
  <meta name="author" content="Kou">
  <link rel="icon" href="/images/icons/favicon-32x32.png" type="image/png" sizes="16x16">
  <link rel="icon" href="/images/icons/favicon-64x64.png" type="image/png" sizes="32x32">
  <link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180">
  <meta rel="mask-icon" href="/images/icons/stun-logo.svg" color="#333333">
  
    <meta rel="msapplication-TileImage" content="/images/icons/favicon-144x144.png">
    <meta rel="msapplication-TileColor" content="#000000">
  

  
<link rel="stylesheet" href="/css/style.css">


  
    
<link rel="stylesheet" href="https://at.alicdn.com/t/font_1445822_p6ry5n7lrr.css">

  

  
    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css">

  

  
    
      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/foundation.min.css" name="highlight-style" mode="light">

      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/solarized-dark.min.css" name="highlight-style" mode="dark">

      
  

  <script>
    var CONFIG = window.CONFIG || {};
    var ZHAOO = window.ZHAOO || {};
    CONFIG = {
      isHome: false,
      fancybox: true,
      pjax: false,
      loading: {
        gif: '/images/theme/loading.gif',
        lottie: ''
      },
      lazyload: {
        enable: true,
        only_post: 'false',
        loading: {
          gif: '/images/theme/loading.gif',
          lottie: ''
        }
      },
      donate: {
        enable: false,
        alipay: '',
        wechat: ''
      },
      galleries: {
        enable: false
      },
      fab: {
        enable: true,
        always_show: false
      },
      carrier: {
        enable: true
      },
      daovoice: {
        enable: false
      },
      preview: {
        background: {
          default: '',
          api: ''
        },
        motto: {
          default: 'もしも僕が主人公なら、僕は人の心が見えて',
          typing: true,
          api: '',
          data_contents: '["data","content"]'
        },
      },
      qrcode: {
        enable: false,
        type: 'url',
        image: '/images/theme/qr-code.png',
      },
      toc: {
        enable: true
      },
      scrollbar: {
        type: 'default'
      },
      notification: {
        enable: false,
        delay: 4500,
        list: '',
        page_white_list: '',
        page_black_list: ''
      },
      search: {
        enable: false,
        path: ''
      }
    }
  </script>

  

  

<meta name="generator" content="Hexo 6.3.0"></head>

<body class="lock-screen">
  <div class="loading" id="loading"></div>
  
    


  <nav class="navbar">
    <div class="left">
      
        <i class="iconfont iconhome j-navbar-back-home"></i>
      
      
      
        <i class="iconfont iconmoono" id="color-toggle" color-toggle="light"></i>
      
      
    </div>
    <div class="center">Language models scale reliably with over-training and on downstream tasks</div>
    <div class="right">
      <i class="iconfont iconmenu j-navbar-menu"></i>
    </div>
    
  </nav>

  
  

<nav class="menu">
  <div class="menu-container">
    <div class="menu-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <ul class="menu-content"><li class="menu-item">
        <a href="/ " class="underline "> home</a>
      </li><li class="menu-item">
        <a href="/archives/ " class="underline "> archives</a>
      </li><li class="menu-item">
        <a href="/about/ " class="underline "> about</a>
      </li></ul>
    
  </div>
</nav>
  <main id="main">
  <div class="article-wrap">
    <div class="row container">
      <div class="col-xl-3"></div>
      <div class="col-xl-6"><article class="article">
  <div class="wrap">
    <section class="head">
  <img   class="lazyload" data-original="/images/theme/post-image.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="  draggable="false">
  <div class="head-mask">
    <h1 class="head-title">Language models scale reliably with over-training and on downstream tasks</h1>
    <div class="head-info">
      <span class="post-info-item"><i class="iconfont iconcalendar"></i>May 26, 2024</span>
      
      <span class="post-info-item"><i class="iconfont iconfont-size"></i>1625</span>
    </div>
  </div>
</section>
    <section class="main">
      <section class="content">
        
        <p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2403.08540">Language models scale reliably with over-training and on downstream tasks</a></p>
<p>This paper investigates the issue of scaling language models and identifies a gap between current scaling research and the actual training and evaluation of language models. Existing scaling research is primarily based on computing the optimal training environment. However, in practice, models often undergo excessive training to reduce inference costs. Additionally, scaling laws typically predict the loss of the next token, but models are ultimately compared based on the performance of downstream tasks. This paper addresses these two issues.</p>
<p>The authors established a test platform consisting of 104 models, with parameter ranges from 0.011B to 6.9B. Different numbers of tokens were used during training, and training was conducted on three data distributions. First, the authors studied the scaling issue in environments of excessive training. By fitting the scaling laws of model parameter count and the ratio of training tokens to parameters, the authors were able to predict the validation loss of a model with 1.4B parameters and 900B tokens (i.e., trained excessively by a factor of 32) and a model with 6.9B parameters and 138B tokens (i.e., trained excessively by a factor of 300), requiring only 300 and 20 times the computational experiments, respectively.</p>
<p>Secondly, the authors associated the perplexity of language models with the performance of downstream tasks using power laws and used this to predict the top-1 error rates of these two models on multiple downstream tasks, requiring only 20 times the computational experiments.</p>

      </section>
      <section class="extra">
        
          
        
        
        
        
  <nav class="nav">
    <a></a>
    <a href="/2024/05/22/Weekly%20Contest%20398/">Weekly Contest 398<i class="iconfont iconright"></i></a>
  </nav>

      </section>
      
    </section>
  </div>
</article></div>
      <div class="col-xl-3">
        
          
  <aside class="toc-wrap">
    <h3 class="toc-title">Index</h3>
    
  </aside>

        
      </div>
    </div>
  </div>
</main>
  

  <footer class="footer">
    <div class="footer-social"><a href="mailto:Avidya@ieee.org " target="_blank"
  class="footer-social-item" onMouseOver="this.style.color=#FF3B00"
  onMouseOut="this.style.color='#33333D'">
  <i class="iconfont  iconmail"></i>
  </a></div>
    
  </footer>
  
      <div class="fab fab-plus">
    <i class="iconfont iconplus"></i>
  </div>
  
  
  
  <div class="fab fab-up">
    <i class="iconfont iconcaret-up"></i>
  </div>
  
  
  
    
<script src="/js/color-mode.js"></script>

  
  
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>

<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>





  
<script src="https://cdn.bootcdn.net/ajax/libs/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>




  
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>








<script src="/js/utils.js"></script>
<script src="/js/script.js"></script>







  <script>
    (function () {
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>












</html>